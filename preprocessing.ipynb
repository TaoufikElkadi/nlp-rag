{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5a0e0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/taouf/Documents/TUD/Courses/Year2/NLP/Project/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from dexter.data.loaders.RetrieverDataset import RetrieverDataset\n",
    "from dexter.config.constants import Split\n",
    "from dexter.data.datastructures.hyperparameters.dpr import DenseHyperParams\n",
    "from dexter.retriever.dense.Contriever import Contriever\n",
    "from dexter.utils.metrics.SimilarityMatch import CosineSimilarity\n",
    "from dexter.utils.metrics.retrieval.RetrievalMetrics import RetrievalMetrics\n",
    "from dexter.llms.llm_engine_orchestrator import LLMEngineOrchestrator\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18ca20f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Testing Data Loading\n",
      "================================================================================\n",
      "\n",
      "1. Loading dataset...\n",
      "   - Dataset: musiqueqa\n",
      "   - Corpus: wiki-musiqueqa-corpus\n",
      "   - Split: DEV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading passages: 100%|██████████| 563424/563424 [00:00<00:00, 775038.02it/s]\n",
      "Transforming passage dataset: 100%|██████████| 563424/563424 [00:01<00:00, 502835.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harley-Davidson Harley-Davidson\n",
      "KeysView(<Section: Data-Path>)\n",
      "12576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [02:05<00:00,  9.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✓ Loader initialized successfully!\n",
      "\n",
      "2. Extracting queries, qrels, and corpus...\n",
      "   ✓ Data loaded successfully!\n",
      "\n",
      "3. Data Statistics:\n",
      "   - Total queries: 1200\n",
      "   - Total qrels: 1200\n",
      "   - Corpus size: 563424 documents\n",
      "\n",
      "4. First 1200 queries (your test set):\n",
      "   - Test set size: 1200 queries\n",
      "\n",
      "5. Sample query (first one):\n",
      "   - ID: 8813f87c0bdd11eba7f7acde48001122\n",
      "   - Question: Who is the mother of the director of film Polish-Russian War (Film)?...\n",
      "\n",
      "6. Sample corpus document (first one):\n",
      "   - ID: 0\n",
      "   - Title: Ishberda\n",
      "   - Text preview: Ishberda is a rural locality( a selo) and the administrative center of Ishberdinsky Selsoviet, Bayma...\n",
      "\n",
      "================================================================================\n",
      "✓ SUCCESS! Your data is loaded correctly.\n",
      "================================================================================\n",
      "\n",
      "You're ready to proceed to Step 3: Running retrieval experiments!\n"
     ]
    }
   ],
   "source": [
    "# test_data_loading.py\n",
    "\"\"\"\n",
    "Simple script to test if data loading works correctly\n",
    "\"\"\"\n",
    "\n",
    "from dexter.data.loaders.RetrieverDataset import RetrieverDataset\n",
    "from dexter.config.constants import Split\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Testing Data Loading\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n1. Loading dataset...\")\n",
    "print(\"   - Dataset: musiqueqa\")\n",
    "print(\"   - Corpus: wiki-musiqueqa-corpus\")\n",
    "print(\"   - Split: DEV\")\n",
    "\n",
    "try:\n",
    "    loader = RetrieverDataset(\n",
    "        \"wikimultihopqa\",\n",
    "        \"wiki-musiqueqa-corpus\",\n",
    "        \"evaluation/config.ini\",\n",
    "        Split.DEV,\n",
    "        tokenizer=None  # None = faster loading, just get raw data\n",
    "    )\n",
    "    \n",
    "    print(\"   ✓ Loader initialized successfully!\")\n",
    "    \n",
    "    print(\"\\n2. Extracting queries, qrels, and corpus...\")\n",
    "    queries, qrels, corpus = loader.qrels()\n",
    "    \n",
    "    print(f\"   ✓ Data loaded successfully!\")\n",
    "    print(f\"\\n3. Data Statistics:\")\n",
    "    print(f\"   - Total queries: {len(queries)}\")\n",
    "    print(f\"   - Total qrels: {len(qrels)}\")\n",
    "    print(f\"   - Corpus size: {len(corpus)} documents\")\n",
    "    \n",
    "    print(f\"\\n4. First 1200 queries (your test set):\")\n",
    "    test_queries = queries[:1200]\n",
    "    print(f\"   - Test set size: {len(test_queries)} queries\")\n",
    "    \n",
    "    print(f\"\\n5. Sample query (first one):\")\n",
    "    first_query = queries[0]\n",
    "    print(f\"   - ID: {first_query.id()}\")\n",
    "    print(f\"   - Question: {first_query.text()[:100]}...\")\n",
    "    \n",
    "    print(f\"\\n6. Sample corpus document (first one):\")\n",
    "    first_doc = corpus[0]\n",
    "    print(f\"   - ID: {first_doc.id()}\")\n",
    "    print(f\"   - Title: {first_doc.title()}\")\n",
    "    print(f\"   - Text preview: {first_doc.text()[:100]}...\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"✓ SUCCESS! Your data is loaded correctly.\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\nYou're ready to proceed to Step 3: Running retrieval experiments!\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"\\n✗ ERROR: File not found - {e}\")\n",
    "    print(\"\\nCheck that:\")\n",
    "    print(\"  1. config.ini line 6 is: musiqueqa = data\")\n",
    "    print(\"  2. config.ini line 9 is: wiki-musiqueqa-corpus = data/wiki_musique_corpus.json\")\n",
    "    print(\"  3. Files exist: data/dev.json and data/wiki_musique_corpus.json\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n✗ ERROR: {type(e).__name__}: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e950f2a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading passages: 100%|██████████| 563424/563424 [00:00<00:00, 1199990.02it/s]\n",
      "Transforming passage dataset: 100%|██████████| 563424/563424 [00:01<00:00, 519926.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harley-Davidson Harley-Davidson\n",
      "KeysView(<Section: Data-Path>)\n",
      "12576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [02:20<00:00,  8.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1200 queries, 563424 documents\n"
     ]
    }
   ],
   "source": [
    "from dexter.data.loaders.RetrieverDataset import RetrieverDataset\n",
    "from dexter.config.constants import Split\n",
    "from dexter.data.datastructures.hyperparameters.dpr import DenseHyperParams\n",
    "from dexter.retriever.dense.Contriever import Contriever\n",
    "from dexter.utils.metrics.SimilarityMatch import CosineSimilarity\n",
    "from dexter.utils.metrics.retrieval.RetrievalMetrics import RetrievalMetrics\n",
    "import json\n",
    "\n",
    "print(\"Loading data...\")\n",
    "loader = RetrieverDataset(\"wikimultihopqa\", \"wiki-musiqueqa-corpus\", \n",
    "                          \"evaluation/config.ini\", Split.DEV, tokenizer=None)\n",
    "queries, qrels, corpus = loader.qrels()\n",
    "\n",
    "print(f\"Loaded {len(queries)} queries, {len(corpus)} documents\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfb04ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS available: True\n",
      "MPS built: True\n",
      "2.9.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"MPS available: {torch.backends.mps.is_available()}\")\n",
    "print(f\"MPS built: {torch.backends.mps.is_built()}\")\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c725aa60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding queries once...\n",
      "token_emb torch.Size([1200, 35, 768])\n",
      "sentence_emb torch.Size([1200, 768])\n",
      "Encoding corpus once (563424 documents)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/563424 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting encoding of contexts....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "563456it [19:07:55,  8.18it/s]                                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_embeddings torch.Size([563424, 768])\n",
      "Query embeddings shape: torch.Size([1200, 768])\n",
      "Corpus embeddings shape: torch.Size([563424, 768])\n",
      "\n",
      "================================================================================\n",
      "Computing top-1 from embeddings...\n",
      "================================================================================\n",
      "Similarity scores shape: torch.Size([1200, 563424])\n",
      "\n",
      "Results for k=1:\n",
      "({'NDCG@1': 0.58083}, {'MAP@1': 0.0584}, {'Recall@1': 0.0584}, {'P@1': 0.58083})\n",
      "\n",
      "================================================================================\n",
      "Computing top-3 from embeddings...\n",
      "================================================================================\n",
      "Similarity scores shape: torch.Size([1200, 563424])\n",
      "\n",
      "Results for k=3:\n",
      "({'NDCG@3': 0.34511}, {'MAP@3': 0.07699}, {'Recall@3': 0.09721}, {'P@3': 0.32222})\n",
      "\n",
      "================================================================================\n",
      "Computing top-5 from embeddings...\n",
      "================================================================================\n",
      "Similarity scores shape: torch.Size([1200, 563424])\n",
      "\n",
      "Results for k=5:\n",
      "({'NDCG@5': 0.28171}, {'MAP@5': 0.08854}, {'Recall@5': 0.12153}, {'P@5': 0.24167})\n",
      "\n",
      "================================================================================\n",
      "DONE! Retrieval results saved.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Initialize Contriever \n",
    "config = DenseHyperParams(\n",
    "    query_encoder_path=\"facebook/contriever\",\n",
    "    document_encoder_path=\"facebook/contriever\",\n",
    "    batch_size=64,  # Increased from 32 for faster processing\n",
    "    show_progress_bar=True\n",
    ")\n",
    "retriever = Contriever(config)\n",
    "similarity = CosineSimilarity()\n",
    "\n",
    "# CHANGE 1: Use smaller corpus for faster testing (or full corpus if you have time)\n",
    "# corpus_subset = corpus[:100]  \n",
    "corpus_subset = corpus  # Uncomment this line to use full corpus later\n",
    "\n",
    "# Encode once, reuse for all k values\n",
    "print(\"Encoding queries once...\")\n",
    "query_embeddings = retriever.encode_queries(queries, batch_size=64)\n",
    "\n",
    "print(f\"Encoding corpus once ({len(corpus_subset)} documents)...\")\n",
    "corpus_embeddings = retriever.encode_corpus(corpus_subset)\n",
    "\n",
    "print(\"Query embeddings shape:\", query_embeddings.shape)\n",
    "print(\"Corpus embeddings shape:\", corpus_embeddings.shape)\n",
    "\n",
    "# Now retrieve for different k values using pre-computed embeddings\n",
    "results = {}\n",
    "for k in [1, 3, 5]:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Computing top-{k} from embeddings...\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Compute similarity scores\n",
    "    cos_scores = similarity.evaluate(query_embeddings, corpus_embeddings)\n",
    "    print(f\"Similarity scores shape: {cos_scores.shape}\")\n",
    "    \n",
    "    # Get top-k indices for each query\n",
    "    response = {}\n",
    "    for query_idx in range(len(queries)):\n",
    "        query_id = queries[query_idx].id()\n",
    "        # Get top-k document indices and scores\n",
    "        top_k_scores, top_k_indices = torch.topk(cos_scores[query_idx], k)\n",
    "        \n",
    "        # Store results\n",
    "        response[query_id] = {\n",
    "            str(corpus_subset[idx.item()].id()): float(score.item())  # Use corpus ID, not index\n",
    "            for idx, score in zip(top_k_indices, top_k_scores)\n",
    "        }\n",
    "    \n",
    "    # Evaluate\n",
    "    metrics = RetrievalMetrics(k_values=[k])\n",
    "    eval_results = metrics.evaluate_retrieval(qrels=qrels, results=response)\n",
    "    \n",
    "    results[k] = {'retrieval': response, 'metrics': eval_results}\n",
    "    print(f\"\\nResults for k={k}:\")\n",
    "    print(eval_results)\n",
    "    \n",
    "    # Save retrieval results\n",
    "    with open(f'retrieval_k{k}.json', 'w') as f:\n",
    "        json.dump(response, f)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"DONE! Retrieval results saved.\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dce539",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

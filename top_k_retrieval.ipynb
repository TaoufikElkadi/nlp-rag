{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4e61af5",
   "metadata": {},
   "source": [
    "# LLM exact match performance with top [1,3,5] retrieved contexts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d71b6cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from dexter.llms.llm_engine_orchestrator import LLMEngineOrchestrator\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dexter.config.constants import Split\n",
    "from dexter.data.loaders.RetrieverDataset import RetrieverDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07f3519b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_instance = LLMEngineOrchestrator()\n",
    "llm_instance = config_instance.get_llm_engine(data=\"\",llm_class=\"llama\",model_name=\"llama-3.1-8b-instant\")\n",
    "\n",
    "system_prompt = \"\"\"You are a precise answering assistant. You will be provided with a [Question] and a [Context] to help you answer the question. The [Context] consists of documents with a [Title] and [Text].\n",
    "1. Answer the question using ONLY the provided context.\n",
    "2. Do not use full sentences. Provide only the exact answer entity or phrase.\n",
    "3. Do not add conversational filler like \"The answer is\" or \"Based on the context\".\n",
    "4. If the answer is not contained within the context, respond with \"Not Answerable\".\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe0c5b7",
   "metadata": {},
   "source": [
    "### Run inference for top-k = [1,3,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6e24ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading questions...\n",
      "Loading corpus...\n",
      "Starting Inference for k=1...\n",
      "Loading retrieval results from retrieval_k1.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 1200/1200 [03:33<00:00,  5.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved predictions to  predictions_k1.json\n",
      "Loading retrieval results from retrieval_k3.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 1200/1200 [04:13<00:00,  4.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved predictions to  predictions_k3.json\n",
      "Loading retrieval results from retrieval_k5.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 1200/1200 [04:59<00:00,  4.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved predictions to  predictions_k5.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def load_data(dev_path, corpus_path):\n",
    "    \"\"\"Loads question data and corpus data.\"\"\"\n",
    "    print(\"Loading questions...\")\n",
    "    with open(dev_path, 'r') as f:\n",
    "        questions = json.load(f)\n",
    "    # Map id -> question object for easy lookup\n",
    "    questions_map = {q['_id']: q for q in questions}\n",
    "    \n",
    "    print(\"Loading corpus...\")\n",
    "    with open(corpus_path, 'r') as f:\n",
    "        corpus = json.load(f)\n",
    "        \n",
    "    return questions_map, corpus\n",
    "\n",
    "def load_retrieval_results(retrieval_path):\n",
    "    \"\"\"Loads retrieval results (question_id -> {doc_id: score}).\"\"\"\n",
    "    print(f\"Loading retrieval results from {retrieval_path}...\")\n",
    "    with open(retrieval_path, 'r') as f:\n",
    "        retrieval = json.load(f)\n",
    "    return retrieval\n",
    "\n",
    "def run_inference_loop(llm_engine, retrieval_path, questions_map, corpus, system_prompt, max_samples=None):\n",
    "    \"\"\"\n",
    "    Runs inference on questions using retrieved contexts.\n",
    "    \"\"\"\n",
    "    retrieval_results = load_retrieval_results(retrieval_path)\n",
    "    predictions = {}\n",
    "    \n",
    "    count = 0\n",
    "    # Create prompts and query LLM\n",
    "    for q_id, contexts in tqdm(retrieval_results.items(), desc=\"Running Inference\"):\n",
    "        if max_samples and count >= max_samples:\n",
    "            break\n",
    "            \n",
    "        if q_id not in questions_map:\n",
    "            continue\n",
    "            \n",
    "        # 1. Get Question Text\n",
    "        question_text = questions_map[q_id]['question']\n",
    "        \n",
    "        context_str = \"\"\n",
    "        for doc_id in contexts:\n",
    "            if doc_id in corpus:\n",
    "                doc = corpus[doc_id]\n",
    "                title = doc.get('title', '')\n",
    "                text = doc.get('text', '')\n",
    "                context_str += f\"[Title]: {title}\\n[Text]: {text}\\n\\n\"\n",
    "        \n",
    "        # 3. Construct System and User Prompts\n",
    "        user_prompt = f\"[Context]:\\n{context_str}\\n\\n[Question]: {question_text}\"\n",
    "        \n",
    "        # 4. Call LLM (using the instance you initialized earlier)\n",
    "        try:\n",
    "            answer = llm_engine.get_llama_completion(system_prompt, user_prompt)\n",
    "            predictions[q_id] = answer\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {q_id}: {e}\")\n",
    "            predictions[q_id] = \"ERROR\"\n",
    "            \n",
    "        count += 1\n",
    "        \n",
    "    return predictions\n",
    "\n",
    "# --- EXECUTION BLOCK ---\n",
    "\n",
    "# 1. Load Data\n",
    "# Ensure these paths are correct relative to your notebook\n",
    "questions_map, corpus = load_data('data/dev.json', 'data/wiki_musique_corpus.json')\n",
    "\n",
    "# 2. Run Inference (Example for k=1)\n",
    "# llm_instance is the variable corresponding to your initialized LlamaEngine\n",
    "print(\"Starting Inference for k=1...\")\n",
    "top_k = [1,3,5]\n",
    "for k in top_k:\n",
    "    predictions_k = run_inference_loop(\n",
    "        llm_instance, \n",
    "        f'retrieval_k{k}.json', \n",
    "        questions_map, \n",
    "        corpus,\n",
    "        system_prompt, \n",
    "         # Remove this limit to run on all data\n",
    "    )\n",
    "\n",
    "    output_file = f'predictions_k{k}.json'\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(predictions_k, f)\n",
    "    print(\"saved predictions to \", output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7b6a5e",
   "metadata": {},
   "source": [
    "### Run inference for oracle contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0628e441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Inference with Oracle Contexts...\n",
      "Loading dev.json for oracle contexts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference (Oracle): 100%|██████████| 1200/1200 [06:29<00:00,  3.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved predictions to predictions_oracle.json\n"
     ]
    }
   ],
   "source": [
    "def run_inference_with_oracle_contexts(llm_engine, dev_path, system_prompt, max_samples=None):\n",
    "    \"\"\"\n",
    "    Runs inference on questions using oracle contexts from dev.json.\n",
    "    Oracle contexts are the ground-truth supporting documents provided in the dataset.\n",
    "    \"\"\"\n",
    "    print(\"Loading dev.json for oracle contexts...\")\n",
    "    with open(dev_path, 'r') as f:\n",
    "        questions = json.load(f)\n",
    "    \n",
    "    predictions = {}\n",
    "    count = 0\n",
    "    \n",
    "    for q in tqdm(questions[:1200], desc=\"Running Inference (Oracle)\"):\n",
    "        if max_samples and count >= max_samples:\n",
    "            break\n",
    "        \n",
    "        q_id = q['_id']\n",
    "        question_text = q['question']\n",
    "        \n",
    "        # Build context string from oracle contexts\n",
    "        # Each context entry is [title, [list of paragraph strings]]\n",
    "        context_str = \"\"\n",
    "        for ctx in q['context']:\n",
    "            title = ctx[0]\n",
    "            paragraphs = ctx[1]\n",
    "            text = \" \".join(paragraphs)\n",
    "            context_str += f\"[Title]: {title}\\n[Text]: {text}\\n\\n\"\n",
    "        \n",
    "        user_prompt = f\"[Context]:\\n{context_str}\\n\\n[Question]: {question_text}\"\n",
    "        \n",
    "        try:\n",
    "            answer = llm_engine.get_llama_completion(system_prompt, user_prompt)\n",
    "            predictions[q_id] = answer\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {q_id}: {e}\")\n",
    "            predictions[q_id] = \"ERROR\"\n",
    "        \n",
    "        count += 1\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Run inference with oracle contexts\n",
    "print(\"Starting Inference with Oracle Contexts...\")\n",
    "predictions_oracle = run_inference_with_oracle_contexts(\n",
    "    llm_instance,\n",
    "    'data/dev.json',\n",
    "    system_prompt\n",
    ")\n",
    "\n",
    "output_file = 'predictions_oracle.json'\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(predictions_oracle, f)\n",
    "print(\"Saved predictions to\", output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d431d5e8",
   "metadata": {},
   "source": [
    "### Evaluate experiments with ExactMatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "basof981p",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Exact Match Evaluation Results\n",
      "==================================================\n",
      "Top-1 Retrieved: 0.0400 (1200 samples)\n",
      "Top-3 Retrieved: 0.0692 (1200 samples)\n",
      "Top-5 Retrieved: 0.0742 (1200 samples)\n",
      "Oracle Contexts: 0.2675 (1200 samples)\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Experiment</th>\n",
       "      <th>Exact Match</th>\n",
       "      <th>Samples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Top-1 Retrieved</td>\n",
       "      <td>0.0400</td>\n",
       "      <td>1200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Top-3 Retrieved</td>\n",
       "      <td>0.0692</td>\n",
       "      <td>1200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Top-5 Retrieved</td>\n",
       "      <td>0.0742</td>\n",
       "      <td>1200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Oracle Contexts</td>\n",
       "      <td>0.2675</td>\n",
       "      <td>1200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Experiment Exact Match  Samples\n",
       "0  Top-1 Retrieved      0.0400     1200\n",
       "1  Top-3 Retrieved      0.0692     1200\n",
       "2  Top-5 Retrieved      0.0742     1200\n",
       "3  Oracle Contexts      0.2675     1200"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dexter.utils.metrics.ExactMatch import ExactMatch\n",
    "\n",
    "def evaluate_predictions(predictions_path, dev_path):\n",
    "    \"\"\"\n",
    "    Evaluates predictions against ground truth answers using ExactMatch.\n",
    "    Returns the accuracy (proportion of exact matches).\n",
    "    \"\"\"\n",
    "    # Load predictions\n",
    "    with open(predictions_path, 'r') as f:\n",
    "        predictions = json.load(f)\n",
    "    \n",
    "    # Load ground truth\n",
    "    with open(dev_path, 'r') as f:\n",
    "        questions = json.load(f)\n",
    "    \n",
    "    # Create ground truth map\n",
    "    ground_truth = {q['_id']: q['answer'] for q in questions}\n",
    "    \n",
    "    # Evaluate\n",
    "    metric = ExactMatch()\n",
    "    scores = []\n",
    "    \n",
    "    for q_id, pred_answer in predictions.items():\n",
    "        if q_id in ground_truth:\n",
    "            gt_answer = ground_truth[q_id]\n",
    "            score = metric.evaluate(pred_answer, gt_answer)\n",
    "            scores.append(score)\n",
    "    \n",
    "    accuracy = sum(scores) / len(scores) if scores else 0\n",
    "    return accuracy, len(scores)\n",
    "\n",
    "# Evaluate all prediction files\n",
    "dev_path = 'data/dev.json'\n",
    "prediction_files = [\n",
    "    ('Top-1 Retrieved', 'predictions_k1.json'),\n",
    "    ('Top-3 Retrieved', 'predictions_k3.json'),\n",
    "    ('Top-5 Retrieved', 'predictions_k5.json'),\n",
    "    ('Oracle Contexts', 'predictions_oracle.json'),\n",
    "]\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Exact Match Evaluation Results\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "results = []\n",
    "for name, pred_file in prediction_files:\n",
    "    try:\n",
    "        accuracy, num_samples = evaluate_predictions(pred_file, dev_path)\n",
    "        results.append({'Experiment': name, 'Exact Match': f'{accuracy:.4f}', 'Samples': num_samples})\n",
    "        print(f\"{name}: {accuracy:.4f} ({num_samples} samples)\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"{name}: File not found ({pred_file})\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Display as DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
